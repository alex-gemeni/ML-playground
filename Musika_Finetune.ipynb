{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alex-gemeni/ML-playground/blob/main/Musika_Finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Musika: Generate Stereo Waveform Music in an Instant! \n",
        "\n",
        "In this notebook you will train a Musika system on your own music dataset by finetuning a pretrained model, and then share it on the [Musika Library](https://huggingface.co/musika)!\n",
        "\n",
        "To train a model from scratch (for better results) take a look at the github repo: https://github.com/marcoppasini/musika\n",
        "\n",
        "![Musika Logo](https://marcoppasini.github.io/logo.png)\n"
      ],
      "metadata": {
        "id": "I3vAEjWWI2On"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Setup requirements\n",
        "\n",
        "!wget -O mini.sh https://repo.anaconda.com/miniconda/Miniconda3-py39_4.9.2-Linux-x86_64.sh\n",
        "!chmod +x mini.sh\n",
        "!bash ./mini.sh -b -f -p /usr/local\n",
        "!conda install -q -y jupyter\n",
        "!conda install -q -y google-colab -c conda-forge\n",
        "!python -m ipykernel install --name \"py39\" --user\n",
        "\n",
        "!git clone https://github.com/marcoppasini/musika.git\n",
        "%cd musika\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "r3sLiMIs8If3",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Have a folder ready with a collection of audio files and either upload it to Colab or to your Google Drive (connect to it by executing the next cell). Be aware that single audio files need to be at least 50 seconds long to be properly preprocessed!"
      ],
      "metadata": {
        "id": "SiAZ05cXL4d4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title (Optional) Mount Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "FqyUJuM7LuBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Encode audio files to training samples\n",
        "\n",
        "#@markdown Specify the path of the audio files to use preprocess for training\n",
        "files_path = \"/content/metal\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown (Optional) If you wish to save encodings to a different folder (maybe to your google drive to re-use them later) specify a path here\n",
        "save_path = \"/content/audio_encodings_folder\" #@param {type:\"string\"}\n",
        "\n",
        "!python musika_encode.py --files_path {files_path} --save_path {save_path}"
      ],
      "metadata": {
        "id": "aMYRN_OiM7P6",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Finetune a Musika model\n",
        "\n",
        "#@markdown Specify the pretrained model to finetune\n",
        "base_model = \"misc\" #@param [\"misc\", \"techno\"]\n",
        "\n",
        "#@markdown Specify the path of the encoded training samples from the previous cell\n",
        "train_path = \"/content/audio_encodings_folder\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown (Optional) Number of epochs of the training process. The ideal number of epochs largely depends on the complexity of the training music domain: a number between 5 and 30 is recommended. One epochs always corresponds to about 10'000 iterations. \n",
        "epochs = 5 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown (Optional) If you wish to use a different learning rate from the default one, change the following value\n",
        "learning_rate = 0.00004 #@param {type:\"number\"}\n",
        "\n",
        "#@markdown __IMPORTANT!__\n",
        "\n",
        "#@markdown Click on the public URL to open a Gradio Interface. You can then test the model while it is training (the model used in the demo is updated after each epoch). Feel free to interrupt training earlier if the model produces good results! A tensorboard interface will be displayed: click on Settings, then tick Reload data to monitor training behavior.\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs\n",
        "\n",
        "!python musika_train.py --train_path {train_path} --load_path /content/musika/checkpoints/{base_model} --lr {learning_rate} --epochs {epochs} --share_gradio True"
      ],
      "metadata": {
        "id": "WazxgO2YEhgp",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Test your model\n",
        "\n",
        "#@markdown A Gradio interface will be created to test your trained model. Click on the public URL to access it.\n",
        "\n",
        "#@markdown (Optional) By default, the last checkpoint in the musika/checkpoints folder will be selected. The path is in the form: /content/musika/checkpoints/MUSIKA_latlen_X_latdepth_X_sr_X_time_X-X/MUSIKA_iterations-Xk_losses-X-X-X\n",
        "checkpoint_path = \"\" #@param {type:\"string\"}\n",
        "\n",
        "import os\n",
        "\n",
        "if(not checkpoint_path):\n",
        "  directory = '/content/musika/checkpoints'\n",
        "  exp_path = max([os.path.join(directory,d) for d in os.listdir(directory)], key=os.path.getmtime)\n",
        "  checkpoint_path = max([os.path.join(exp_path,d) for d in os.listdir(exp_path)], key=os.path.getmtime)\n",
        "print(f'Testing model in {checkpoint_path}')\n",
        "\n",
        "!python musika_test.py --load_path {checkpoint_path} --share_gradio True"
      ],
      "metadata": {
        "id": "sDPZLIZEMk7L",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Generate and save samples\n",
        "\n",
        "#@markdown Save a number of samples to a folder (for example on your Google Drive) and listen to them later.\n",
        "\n",
        "#@markdown Number of samples to generate\n",
        "num_samples = 10 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown Length in seconds of each generated sample\n",
        "seconds = 120 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown Where generated samples are saved. If it does not exist, it will be automatically created.\n",
        "save_path = \"/content/generations\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown (Optional) By default, the last checkpoint in the musika/checkpoints folder will be selected. The path is in the form: /content/musika/checkpoints/MUSIKA_latlen_X_latdepth_X_sr_X_time_X-X/MUSIKA_iterations-Xk_losses-X-X-X\n",
        "checkpoint_path = \"\" #@param {type:\"string\"}\n",
        "\n",
        "import os\n",
        "\n",
        "if(not checkpoint_path):\n",
        "  directory = '/content/musika/checkpoints'\n",
        "  exp_path = max([os.path.join(directory,d) for d in os.listdir(directory)], key=os.path.getmtime)\n",
        "  checkpoint_path = max([os.path.join(exp_path,d) for d in os.listdir(exp_path)], key=os.path.getmtime)\n",
        "print(f'Using model in {checkpoint_path}')\n",
        "\n",
        "!python musika_generate.py --load_path {checkpoint_path} --num_samples {num_samples} --seconds {seconds} --save_path {save_path}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "C3pR7yXdfWFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Login to the Huggingface Hub\n",
        "#@markdown Input your Huggingface __write access__ token. To create one, go [here](https://huggingface.co/settings/tokens) and specify __write__ for the token role.\n",
        "\n",
        "import sys\n",
        "_ = (sys.path.append(\"/usr/local/lib/python3.9/site-packages\"))\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "!git config --global credential.helper store\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "3qvz9M9AZLaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Save your model to the HuggingFace Hub\n",
        "\n",
        "#@markdown You can either save your trained model to the public Musika models library, or save it privately.\n",
        "\n",
        "#@markdown Specify a name for your Musika model\n",
        "musika_model_name = \"musika_metal\" #@param {type:\"string\"}\n",
        "#@markdown Where to save your model. You can allow other users to easily find and utilize your model by saving it in the Huggingface Musika organization, or you can save it as a private model on your account.\n",
        "save_location = \"musika_organization\" #@param [\"musika_organization\", \"private\"]\n",
        "\n",
        "#@markdown (Optional) By default, the last checkpoint in the musika/checkpoints folder will be selected. The path is in the form: /content/musika/checkpoints/MUSIKA_latlen_X_latdepth_X_sr_X_time_X-X/MUSIKA_iterations-Xk_losses-X-X-X\n",
        "checkpoint_path = \"\" #@param {type:\"string\"}\n",
        "\n",
        "# code inspired from https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_training.ipynb\n",
        "\n",
        "from slugify import slugify\n",
        "from huggingface_hub import hf_hub_download\n",
        "from huggingface_hub import HfApi, HfFolder, CommitOperationAdd\n",
        "from huggingface_hub import create_repo\n",
        "from IPython.display import display_markdown\n",
        "import os\n",
        "\n",
        "if(not checkpoint_path):\n",
        "  directory = '/content/musika/checkpoints'\n",
        "  exp_path = max([os.path.join(directory,d) for d in os.listdir(directory)], key=os.path.getmtime)\n",
        "  checkpoint_path = max([os.path.join(exp_path,d) for d in os.listdir(exp_path)], key=os.path.getmtime)\n",
        "print(f'Saving model in {checkpoint_path}')\n",
        "\n",
        "api = HfApi()\n",
        "your_username = api.whoami()[\"name\"]\n",
        "\n",
        "if(save_location == \"musika_organization\"):\n",
        "  repo_id = f\"musika/{slugify(musika_model_name)}\"\n",
        "  #Join the Musika organization if you aren't part of it already\n",
        "  !curl -X POST -H 'Authorization: Bearer '$hf_token -H 'Content-Type: application/json' https://huggingface.co/organizations/musika/share/XjEqSfEgAwKDKTZyEzcyHvCbPaJYBcEAcz\n",
        "else:\n",
        "  repo_id = f\"{your_username}/{slugify(musika_model_name)}\"\n",
        "\n",
        "with open(HfFolder.path_token, 'r') as fin: hf_token = fin.read();\n",
        "\n",
        "readme_text = f'''---\n",
        "license: mit\n",
        "tags:\n",
        "- audio\n",
        "- music\n",
        "- generation\n",
        "- tensorflow\n",
        "---\n",
        "\n",
        "# Musika Model: {musika_model_name}\n",
        "## Model provided by: {your_username}\n",
        "\n",
        "Pretrained {musika_model_name} model for the [Musika system](https://github.com/marcoppasini/musika) for fast infinite waveform music generation.\n",
        "Introduced in [this paper](https://arxiv.org/abs/2208.08706).\n",
        "\n",
        "## How to use\n",
        "\n",
        "You can generate music from this pretrained {musika_model_name} model using the notebook available [here](https://colab.research.google.com/drive/1HJWliBXPi-Xlx3gY8cjFI5-xaZgrTD7r).\n",
        "\n",
        "### Model description\n",
        "\n",
        "This pretrained GAN system consists of a ResNet-style generator and discriminator. During training, stability is controlled by adapting the strength of gradient penalty regularization on-the-fly. The gradient penalty weighting term is contained in *switch.npy*. The generator is conditioned on a latent coordinate system to produce samples of arbitrary length. The latent representations produced by the generator are then passed to a decoder which converts them into waveform audio.\n",
        "The generator has a context window of about 12 seconds of audio.\n",
        "'''\n",
        "readme_file = open(\"README.md\", \"w\")\n",
        "readme_file.write(readme_text)\n",
        "readme_file.close()\n",
        "operations = [\n",
        "  CommitOperationAdd(path_in_repo=\"README.md\", path_or_fileobj=\"README.md\"),\n",
        "]\n",
        "create_repo(repo_id, private=True, token=hf_token)\n",
        "api.create_commit(\n",
        "  repo_id=repo_id,\n",
        "  operations=operations,\n",
        "  commit_message=f\"Upload {musika_model_name} readme\",\n",
        "  token=hf_token\n",
        ")\n",
        "api.upload_folder(\n",
        "  folder_path=checkpoint_path,\n",
        "  path_in_repo=\"\",\n",
        "  repo_id=repo_id,\n",
        "  token=hf_token\n",
        ")\n",
        "display_markdown(f'''## Musika model successfully saved! See it [here](https://huggingface.co/{repo_id}). \n",
        "''', raw=True)"
      ],
      "metadata": {
        "id": "1ZSEuho0MW5w",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}